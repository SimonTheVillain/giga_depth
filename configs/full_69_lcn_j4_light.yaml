training:
  name: full_69_lcn_j4 # experiment to create a better mask
  batch_size: 4
  learning_rate: 4.0
  lr_scales: [1.0, 0.5, 0.25, 0.12, 0.01]
  momentum: 0.9
  weight_decay: 1e-5
  alpha_reg: [500, 1000, 2000, 2000, 2000]
  alpha_sigma: [1e-2, 1e-2, 1e-1, 1, 10]
  edge_weight: [0, 1, 2, 4, 20]
  sigma_mode: automask
  gpu_list: [0]
  epochs: 13
  key_epochs: [ 2, 4, 6, 8 ]
  half_precision: True

dataset:
  path: /media/simon/WD/datasets_raw
  is_npy: False
  workers: 8
  tgt_res: [ 1216, 896 ]
  vertical_jitter: 4
  downsample_output: True
  dataset_type: structure_core_unity_sequences
  slice_in:
    start: 0
    height: 896
  slice_out:
    start: 0
    height: 448


backbone:
  load_file: ""
  name: BackboneSliced
  input_channels: 2
  lines: 896
  slices: 1
  channels: []
  channels2: [16, 32, 32, 64, 64]
  norm: batch
  local_contrast_norm: True

regressor:
  load_file: ""
  name: Regv3
  ch_in: 64
  lines: 448
  bb: []
  classes: [16, 12, 10] #1920 classes
  padding: [0, 2, 3]
  class_bb: [[32], [32], [32]]
  superclasses: 480
  ch_reg: [32]
  msk: [32, 32, 16]
  regress_neighbours: 1
  reg_line_div: 1
  c3_line_div: 1 #8 lines share the same weights for the last classification stage. (maybe this is over the top!)
  close_far_separation: True
  sigma_mode: conv

